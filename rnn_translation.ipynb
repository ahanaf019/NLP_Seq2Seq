{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 2.13.0\n",
      "Device: NVIDIA GeForce GTX 1070\n",
      "Compute Capability: (6, 1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import cv2\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "SEED = 225\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(f'Tensorflow Version: {tf.__version__}')\n",
    "gpus = tf.config.experimental.get_device_details(gpus[0])\n",
    "print(f'Device: {gpus[\"device_name\"]}\\nCompute Capability: {gpus[\"compute_capability\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 15000\n",
    "SEQUENCE_LENGTH = 25\n",
    "V_SPLIT = 0.15\n",
    "T_SPLIT = 0.15\n",
    "BATCH_SIZE = 64\n",
    "EMBED_DIMS = 256\n",
    "LATENT_DIMS = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>dst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81158</th>\n",
       "      <td>Tom doesn't think before he speaks.</td>\n",
       "      <td>[start] Tom no piensa antes de hablar. [end]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>Seize him!</td>\n",
       "      <td>[start] ¡Cogedlo! [end]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94483</th>\n",
       "      <td>All three of my sons were born in Boston.</td>\n",
       "      <td>[start] Mis tres hijos nacieron en Boston. [end]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             src  \\\n",
       "81158        Tom doesn't think before he speaks.   \n",
       "738                                   Seize him!   \n",
       "94483  All three of my sons were born in Boston.   \n",
       "\n",
       "                                                    dst  \n",
       "81158      [start] Tom no piensa antes de hablar. [end]  \n",
       "738                             [start] ¡Cogedlo! [end]  \n",
       "94483  [start] Mis tres hijos nacieron en Boston. [end]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_table('./spa-eng/spa.txt', sep='\\t', header=None, names=['src', 'dst'])\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    x = df.iloc[i, 1]\n",
    "    df.iloc[i, 1] = f'[start] {x} [end]'\n",
    "\n",
    "df = df.sample(df.shape[0])\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "strip_chars = string.punctuation + '¿'\n",
    "strip_chars = strip_chars.replace('[', '')\n",
    "strip_chars = strip_chars.replace(']', '')\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(lowercase, f'{re.escape(strip_chars)}', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_vectorization = keras.layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=SEQUENCE_LENGTH,\n",
    ")\n",
    "\n",
    "target_vectorization = keras.layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=SEQUENCE_LENGTH + 1,\n",
    "    standardize=custom_standardization,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Train, Validation and Test set size:', 83275, 17844, 17845)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_lim = int(V_SPLIT * df.shape[0])\n",
    "test_lim = int((V_SPLIT + T_SPLIT) * df.shape[0])\n",
    "\n",
    "val_pairs = df[:val_lim]\n",
    "test_pairs = df[val_lim : test_lim]\n",
    "train_pairs = df[ test_lim : ]\n",
    "\n",
    "'Train, Validation and Test set size:', len(train_pairs), len(val_pairs), len(test_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_src_texts = [ x for x in train_pairs['src'] ]\n",
    "train_dst_texts = [ x for x in train_pairs['dst'] ]\n",
    "\n",
    "source_vectorization.adapt(train_src_texts)\n",
    "target_vectorization.adapt(train_dst_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataset(src, dst):\n",
    "    src = source_vectorization(src)\n",
    "    dst = target_vectorization(dst)\n",
    "    return ({\n",
    "        'source': src,\n",
    "        'destination': dst[ :, : -1],\n",
    "    }, dst[ :, 1 : ])\n",
    "\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    src_txts, dst_txts = pairs['src'], pairs['dst']\n",
    "    src_txts = list(src_txts)\n",
    "    dst_txts = list(dst_txts)\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((src_txts, dst_txts))\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.map(format_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return dataset.shuffle(2048).prefetch(tf.data.AUTOTUNE).cache()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 25) (64, 25) (64, 25)\n"
     ]
    }
   ],
   "source": [
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)\n",
    "\n",
    "for inputs, targets in train_ds.take(1):\n",
    "    print(inputs['source'].shape, inputs['destination'].shape, targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = keras.Input(shape=(None,), dtype='int64', name='source')\n",
    "x = keras.layers.Embedding(input_dim=VOCAB_SIZE, output_dim=EMBED_DIMS, mask_zero=True)(source)\n",
    "\n",
    "encoded_sources = keras.layers.Bidirectional(\n",
    "    keras.layers.GRU(units=LATENT_DIMS), merge_mode='sum'\n",
    ")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_targets = keras.Input(shape=(None,), dtype='int64', name='destination')\n",
    "x = keras.layers.Embedding(input_dim=VOCAB_SIZE, output_dim=EMBED_DIMS, mask_zero=True)(past_targets)\n",
    "\n",
    "decoder_gru = keras.layers.GRU(units=LATENT_DIMS, return_sequences=True)\n",
    "x = decoder_gru(inputs=x, initial_state=encoded_sources)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "target_next_step = keras.layers.Dense(VOCAB_SIZE, activation='softmax')(x)\n",
    "\n",
    "\n",
    "model = keras.Model([source, past_targets], target_next_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1302/1302 [==============================] - 158s 110ms/step - loss: 4.6681 - accuracy: 0.3287 - val_loss: 3.8581 - val_accuracy: 0.4025\n",
      "Epoch 2/15\n",
      "1302/1302 [==============================] - 129s 99ms/step - loss: 3.7435 - accuracy: 0.4233 - val_loss: 3.2612 - val_accuracy: 0.4759\n",
      "Epoch 3/15\n",
      "1302/1302 [==============================] - 131s 100ms/step - loss: 3.2765 - accuracy: 0.4742 - val_loss: 2.9184 - val_accuracy: 0.5191\n",
      "Epoch 4/15\n",
      "1302/1302 [==============================] - 133s 102ms/step - loss: 2.9462 - accuracy: 0.5104 - val_loss: 2.6846 - val_accuracy: 0.5498\n",
      "Epoch 5/15\n",
      "1302/1302 [==============================] - 132s 102ms/step - loss: 2.6877 - accuracy: 0.5390 - val_loss: 2.5186 - val_accuracy: 0.5722\n",
      "Epoch 6/15\n",
      "1302/1302 [==============================] - 130s 100ms/step - loss: 2.4762 - accuracy: 0.5630 - val_loss: 2.3891 - val_accuracy: 0.5917\n",
      "Epoch 7/15\n",
      "1302/1302 [==============================] - 131s 101ms/step - loss: 2.2955 - accuracy: 0.5849 - val_loss: 2.2969 - val_accuracy: 0.6030\n",
      "Epoch 8/15\n",
      "1302/1302 [==============================] - 130s 100ms/step - loss: 2.1418 - accuracy: 0.6043 - val_loss: 2.2186 - val_accuracy: 0.6160\n",
      "Epoch 9/15\n",
      "1302/1302 [==============================] - 129s 99ms/step - loss: 2.0067 - accuracy: 0.6211 - val_loss: 2.1551 - val_accuracy: 0.6250\n",
      "Epoch 10/15\n",
      "1302/1302 [==============================] - 133s 102ms/step - loss: 1.8860 - accuracy: 0.6373 - val_loss: 2.1148 - val_accuracy: 0.6323\n",
      "Epoch 11/15\n",
      "1302/1302 [==============================] - 134s 103ms/step - loss: 1.7777 - accuracy: 0.6519 - val_loss: 2.0745 - val_accuracy: 0.6380\n",
      "Epoch 12/15\n",
      "1302/1302 [==============================] - 131s 101ms/step - loss: 1.6859 - accuracy: 0.6643 - val_loss: 2.0376 - val_accuracy: 0.6438\n",
      "Epoch 13/15\n",
      "1302/1302 [==============================] - 130s 100ms/step - loss: 1.5992 - accuracy: 0.6767 - val_loss: 2.0106 - val_accuracy: 0.6471\n",
      "Epoch 14/15\n",
      "1302/1302 [==============================] - 131s 101ms/step - loss: 1.5238 - accuracy: 0.6874 - val_loss: 1.9897 - val_accuracy: 0.6514\n",
      "Epoch 15/15\n",
      "1302/1302 [==============================] - 130s 100ms/step - loss: 1.4563 - accuracy: 0.6964 - val_loss: 1.9754 - val_accuracy: 0.6523\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "history = model.fit(train_ds, validation_data=val_ds, epochs=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow 2.13 Env",
   "language": "python",
   "name": "tf_2_13_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
